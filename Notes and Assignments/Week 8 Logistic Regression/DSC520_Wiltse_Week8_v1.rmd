
---
title: "DSC 520 Week 8  Logistic Regression Assignment"
author: "Dan Wiltse"
date: "Feb 2nd 2020"
output:
  word_document: default
---
  
  ```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
library("foreign")
library("caTools")
library(ggplot2)

# Get housing data from local file and load into a R data frame
data8 <- read.arff("https://archive.ics.uci.edu/ml/machine-learning-databases/00277/ThoraricSurgery.arff")
data8.2 <- read.csv("http://content.bellevue.edu/cst/dsc/520/id/resources/binary-classifier-data.csv")
```



##Dataset

This dataset contains information on life expectancy in lung cancer patients after surgery.

### a.  Fit a binary logistic regression model to the data set that predicts whether or not the patient survived for one year (the Risk1Y variable) after the surgery. Use the glm() function to perform the logistic regression. See Generalized Linear Models for an example. Include a summary using the summary() function in your results.


```{r}
#Logistic Regression

lgm1 <- glm(Risk1Yr ~ DGN + PRE4 + PRE5 + PRE6 + PRE7 + PRE8 + PRE9 + PRE10 + PRE11 + PRE14 + PRE17 + PRE19 + PRE25 + PRE30 + PRE32 + AGE, data = data8, family = binomial()  )
summary(lgm1)

```

## b. According to the summary, which variables had the greatest effect on the survival rate?

PRE9T, PRE14OC14, pRE17T, PRE30T all had significant values, hence the greatest effect on survival rate.

## c. To compute the accuracy of your model, use the dataset to predict the outcome variable. The percent of correct predictions is the accuracy of your model. What is the accuracy of your model?

88.6%

```{r}
# Split the data into training and validation data sets
split <- sample.split(data8, SplitRatio = 0.8)
split
train <- subset(data8, split == "TRUE")
validate <- subset(data8, split == "FALSE")

# Train model using training data set
lgm2 <- glm(Risk1Yr ~ DGN + PRE4 + PRE5 + PRE6 + PRE7 + PRE8 + PRE9 + PRE10 + PRE11 + PRE14 + PRE17 + PRE19 + PRE25 + PRE30 + PRE32 + AGE, data = train, family = binomial()  )
summary(lgm2)

# Run validation data through the model built on training data
res <- predict(lgm2, validate, type = "response")
res

res2 <-predict(lgm2, train, type = "response")
res2

#Validate model using confusion matrix
confmatrix <- table(Actual_Value=train$Risk1Yr, Predicted_Value = res2 >0.5)
confmatrix

#Accuracy
(confmatrix[[1,1]] + confmatrix[[2,2]])/sum(confmatrix)

```


#8.2
Fit a logistic regression model to the dataset from the previous assignment

## a. What is the accuracy of the logistic regression classifier?

57.5%

```{r}
# Split the data into training and validation data sets
split <- sample.split(data8.2, SplitRatio = 0.8)
split
train <- subset(data8.2, split == "TRUE")
validate <- subset(data8.2, split == "FALSE")


# Train model using training data set
lgm8.2 <- glm(label ~ x +y, data = train, family = binomial())
summary(lgm8.2)

# Run validation data through the model built on training data
res <- predict(lgm8.2, validate, type = "response")
res

res2 <-predict(lgm8.2, train, type = "response")
res2

#Validate model using confusion matrix
confmatrix <- table(Actual_Value=train$label, Predicted_Value = res2 >0.5)
confmatrix

#Accuracy
(confmatrix[[1,1]] + confmatrix[[2,2]])/sum(confmatrix)
```


## b. How does the accuracy of the logistic regression classifier compare to the nearest neighbors algorithm?

The accuracy for the nearest neighbors seemed much higher than the logistic regression accuracy.


```{r}
##Generate a random number that is 90% of the total number of rows in dataset.
ran <- sample(1:nrow(data8.2), 0.9 * nrow(data8.2)) 

##the normalization function is created
nor <-function(x) { (x -min(x))/(max(x)-min(x))   }

##Run nomalization on the last 2 columns of dataset because they are the predictors
knn_norm <- as.data.frame(lapply(data8.2[,c(2,3)], nor))

summary(knn_norm)


##extract training set
knn_train <- knn_norm[ran,] 
##extract testing set
knn_test <- knn_norm[-ran,] 
##extract 1st column of train dataset because it will be used as 'cl' argument in knn function.
knn_target_category <- data8.2[ran,1]
##extract 1st column if test dataset to measure the accuracy
knn_test_category <- data8.2[-ran,1]
##load the package class
library(class)
##run knn function
pr <- knn(knn_train,knn_test,cl=knn_target_category,k=5)

##create confusion matrix
tab <- table(pr,knn_test_category)

##this function divides the correct predictions by total number of predictions that tell us how accurate the model is.

accuracy <- function(x){sum(diag(x)/(sum(rowSums(x)))) * 100}
accuracy(tab)

```

## c. Why is the accuracy of the logistic regression classifier different from that of the nearest neighbors?

Logistic regression works better with linear relationships, and by looking at the plot below, you can see there is a non-linear relationship between the data and the predictor.


```{r}
data_space <-ggplot(data8.2, aes(x = x, y = y, col = label)) + 
  geom_point()

data_space +
  geom_smooth(method = "glm", se = FALSE)
```
